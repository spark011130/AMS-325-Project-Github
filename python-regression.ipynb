{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Modeling: Linear Regression\n",
    "\n",
    "**Overview**\n",
    "- Concept of statistical modeling\n",
    "- Defining statistical models with Patsy\n",
    "- Linear regression\n",
    "- Discrete regression (next lecture)\n",
    "\n",
    "We will use the [statsmodels](https://www.statsmodels.org/stable/index.html) libray which provides classes and functions for defining statistical models and fitting them to observed data, for calculating descriptive statistics and carrying out statistical tests. The api modules collect the publically accessible symbols that the library provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.graphics.api as smg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Patsy](https://patsy.readthedocs.io/en/latest/) library allows us to write statistical models as simple formulas. It is inspired by statiscal software such as R and S. The statmodels library internally uses the Patsy library, so we do not need to access the Patsy's functions directly. However, we will use Patsy for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Problem:** For a set of response(dependent) variables $Y$, and explanatory(independent) variable $X$, we want to find a relationship (model) between $Y$ and $X$:\n",
    "- mathematical model:         $~~~ Y = f(X)$\n",
    "- statistical model:        $~~~ Y = f(X) + \\epsilon~~$ where $\\epsilon$ is a random variable. A model is statistical when the data ${y_i, x_i}$ has an element of uncertainty (e.g. due to measurement noise), which is described as $\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used model is \n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are model parameters and $\\epsilon$ is normally distributed with $0$ mean and variance $\\sigma^2$. \n",
    "* If $X$ is a scalar, the model is known as *simple linear regression*.\n",
    "* If $X$ is a vector, the model is known as *multiple linear regression*.\n",
    "* If $Y$ is a vector, the model is known as *multivariate linear regression*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we form the model, we construct the so-called design matrices $y$ and $X$ such that the regression problem can be written on matrix form:\n",
    "\n",
    "$$\n",
    "  y = X\\beta + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $y$ is the vector (or matrix) of observations, $\\beta$ is a vector of cefficients, and $\\epsilon$ is the residual (error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Suppose the observed values are $y = [1,2,3,4,5]$ with two independent variables with values $x_1 = [6,7,8,9,10]$ and $x_2 = [11,12,13,14,15]$.  Assume we use the linear model $~ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$. (Note: \"*Linear*\" is with respect to the coefficents.) Therefore, the design matrix $X$ is $X = [1, x_1, x_2, x_1x_2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4, 5])\n",
    "x1 = np.array([6, 7, 8, 9, 10])\n",
    "x2 = np.array([11, 12, 13, 14, 15])\n",
    "X = np.vstack([np.ones(5), x1, x2, x1*x2]).T\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X$ and $y$, we can solve for $\\beta$ using least squares method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, res, rank, s_val = np.linalg.lstsq(X, y, rcond=None)\n",
    "print(\"beta = \", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, constructing the design matrix $X$ is very simple. However, it can be more difficult for more complicated models. The `Patsy` library provides a simple [formula](https://patsy.readthedocs.io/en/latest/formulas.html#the-formula-language) language to handle this.\n",
    "\n",
    "First, we create a dictionary that maps the variable names to the corresponding data arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"y\": y, \"x1\": x1, \"x2\": x2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model  $~ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$ with Patsy, we can use the formula `y ~ 1 + x1 + x2 + x1*x2` (leave out the coefficients).  Then we can easily get the design matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices(\"y ~ 1 + x1 + x2 + x1*x2\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ordinary linear regression (`OLS`) class in the `statsmodels` library (instead of `np.linalg.lstsq`) to solve for the parameter vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare this with the answer from np.linalg.lstsq\n",
    "result.params - beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the step of creating the design matrices by using the statmodels formula API (we imported it as `smf`) by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dictionary-like object (e.g. a Pandas data frame) that contains the data\n",
    "df_data = pd.DataFrame(data)\n",
    "#pass the Patsy formula and the object\n",
    "model2 = smf.ols(\"y ~ 1 + x1 + x2 + x1:x2\", df_data)\n",
    "#find the paramters using fit\n",
    "result2 = model2.fit()\n",
    "result2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves us time when we want to add and remove terms in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Simplified) summary of the Patsy formula syntax\n",
    "\n",
    "|Syntax|Example| Description |\n",
    "|:-|:- |:---|\n",
    "|lhs ~ rhs|y ~ x <br>(equivalent to y ~ 1+x) |~ is used to separate LHS (dependent variables) and <br> RHS (independent variables) | \n",
    "|var$*$var| x1$*$x2 <br>(equivalent to x1+x2+x1$*$x2) |An interaction term that implicitly contains all lower-order terms|\n",
    "|var + var| x1 + x2 <br>(equivalent to y ~ 1+x1+x2) |+ denotes the union of terms |\n",
    "|var - var| x1 - x2 <br> |- removes the following term |\n",
    "|var:var| x1:x2 |: denotes a pure interaction term (e.g. $x_1\\cdot x_2$)|\n",
    "\n",
    "For a complete syntax, see the Patsy [documentation](https://patsy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "**Basic workflow for analyzing a statistic model using statsmodels**:\n",
    "1. Create an instance of model class, for example, using `model = sm.MODEL(y,X)` or `model = smf.model(formula, data)` where `MODEL` and `model` are the names of a particular model (e.g. OLS, GLS, Logit, etc)\n",
    "2. Fit the model to the data:  `result = model.fit()`\n",
    "3. Print summary statistics for the result:  `result.summary()`\n",
    "4. Post-process the model fit results by methods and attributes `params`, `resid`, `fittedvalues`, `predict`\n",
    "5. Visualize the result by Matplotlib or `statsmodels.graphics` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (linear regression):\n",
    "Consider fitting a model to generated data whose true value is $ y = 1 + 2x_1 + 3x_2 + 4x_1 x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # number of data points\n",
    "np.random.seed(123456789)  #so that we get the same numbers (don't need this)\n",
    "x1 = np.random.randn(N)\n",
    "x2 = np.random.randn(N)\n",
    "data = pd.DataFrame({\"x1\": x1, \"x2\": x2})\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_true(x1, x2):\n",
    "    return 1  + 2 * x1 + 3 * x2 + 4 * x1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add \"y_true\" column to the DataFrame\n",
    "data[\"y_true\"] = y_true(x1, x2)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a normal-distributed noise to the true values and store the result in the y column \n",
    "e = 0.5*np.random.randn(N)\n",
    "data[\"y\"] = data[\"y_true\"] + e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$, which corresponds to the Patsy formula \"`y ~ x1 + x2`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Create an instance of model class (fit the model to the data using ordinary least square)\n",
    "model = smf.ols(\"y ~ x1 + x2\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Fit the model to the data\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Print summary statistics for the result\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What to look for:*\n",
    "- R-squared:  indicates how well the model fits the data. The value is between 0 and 1. The value 1 corresponds to a perfect fit.  (For our example, the R-squared value of 0.393 is pretty poor.)\n",
    "- The `coef` column contains the model parameters. \n",
    "- The `t` column contains the *t-statistics*:  $t$ = coef/(std err).  If the greater $|t|$, the more likely that the corresponding coefficient is non-zero (which means that it has a significant predictive power). [Recall: the greater $|t|$, the greater the evidence against the null hypothesis. Here the null hypothesis is that the coefficient is $0$.]\n",
    "- The `P>|t|` column contains the p-value:  small p-value (<0.05) indicates that that coefficient is more likely to be non-zero.  [Recall: small p-value means strong evidence against the null hypothesis.]\n",
    "\n",
    "Summary:  R-squared close to 1 => good fit. $~~~$High $t$ or small $p$-value => that coef is far from $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the R-squared directly:\n",
    "result.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by using ordinary least-square regression we assume that the residuals (of the fitted model and the data) is normally distributed.  Before analyzing data, we might not know if this condition is met. However, we can investigate this by using statistical tests (with null hypothesis that the residuals are normally distributed) and/or plotting the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the residual \n",
    "result.resid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the normality of the residual\n",
    "z, p = stats.normaltest(result.resid.values)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $p$-value of `normaltest` is very small, so we can reject the null hypothesis (i.e. the assummption that the residuals are normally-distributed is violated). (Recall that we're considering our first model $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a graphical method (`qqplot`) to check for normality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a graphical method to check for normality. (QQ-plot)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "smg.qqplot(result.resid, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the QQ-plot is not quite linear. This suggests that the observed residuals are unlikely to be a sample of a normal-distributed random variable.\n",
    "Therefore, the first model is not sufficient.  Let's add the interaction term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2st model: $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repeat the steps from the previous analysis (Steps 1-3)\n",
    "del model\n",
    "model = smf.ols(\"y ~ x1 + x2 + x1*x2  \", data)\n",
    "result = model.fit()\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r-squared is very close to 1, indicating a nearly perfect fit.\n",
    "\n",
    "Then we check if the residuals are normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. statisical test\n",
    "z, p = stats.normaltest(result.resid.values)\n",
    "p\n",
    "# p is large (>0.05) => weak evidence against the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. qq-plot \n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "smg.qqplot(result.resid, ax=ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are happy with the fitted model, we can extract the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given values of the indepedent variables ($x_1$, and $x_2$ in this case), we can use the `predict` method to get the prediction (the $y$ value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 50)\n",
    "X1, X2 = np.meshgrid(x, x)\n",
    "new_data = pd.DataFrame({\"x1\": X1.ravel(), \"x2\": X2.ravel()}) # ravel returns a contiguous flattened array.\n",
    "y_pred = result.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result is a vector\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the vector to a square matrix\n",
    "y_pred = y_pred.values.reshape(50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the true data and the fitted model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "def plot_y_contour(ax, Y, title):\n",
    "    c = ax.contourf(X1, X2, Y, 15, cmap=plt.cm.RdBu)\n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=20)\n",
    "    ax.set_title(title)\n",
    "    cb = fig.colorbar(c, ax=ax)\n",
    "    cb.set_label(r\"$y$\", fontsize=20)\n",
    "\n",
    "plot_y_contour(axes[0], y_true(X1, X2), \"true relation\")\n",
    "plot_y_contour(axes[1], y_pred, \"fitted model\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets from R\n",
    "The statmodels provides an interface to load data sets to explore.  See http://www.statsmodels.org/dev/datasets/index.html#available-datasets  for available data sets.\n",
    "\n",
    "As an example, we will load a dataset named \"Icecream\" from the package \"Ecdat\" (https://rdrr.io/cran/Ecdat/man/Icecream.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sm.datasets.get_rdataset(\"Icecream\", \"Ecdat\")\n",
    "#dataset = sm.datasets.get_rdataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this dataset has 4 variables: cons(consumption), income, price, and temp. \n",
    "Say we want to model the consumption as a linear model with price and temperator as indepdent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"cons ~ -1 + price + temp\", data=dataset.data)\n",
    "result = model.fit()\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical tools like plot_fit (regression plot) can give a quick look at our model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "smg.plot_fit(result, 0, ax=ax1)\n",
    "smg.plot_fit(result, 1, ax=ax2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumption seems linearly correlated to the temp but doesn't seem so on the price (it's probably because the price range is quite small). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "51bbcd93b7d995468114f7720787c40b1db854c09905160d6d3ae8803b9e8b34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
